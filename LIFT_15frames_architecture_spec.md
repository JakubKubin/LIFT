# LIFT-15: Long-range Interpolation with Far Temporal context
## Specyfikacja Architektury (15 klatek)

**Wersja:** 2.1  
**Data:** 2025-01  
**Autor:** [Twoje dane]  
**Cel:** Master Thesis - Video Frame Interpolation z rozszerzonym kontekstem czasowym

---

## 1. PrzeglÄ…d ogÃ³lny

### 1.1 Hipoteza badawcza

Wykorzystanie szerokiego kontekstu czasowego (15 klatek vs standardowe 2-4) pozwala na lepszÄ… interpolacjÄ™ klatek wideo, szczegÃ³lnie w scenach z:
- Okluzjami (obiekty przysÅ‚aniajÄ…ce siÄ™ nawzajem)
- Szybkim ruchem
- PowtarzajÄ…cymi siÄ™ wzorcami (koÅ‚a, nogi w ruchu)
- Zmianami oÅ›wietlenia

### 1.2 Konfiguracja wejÅ›cia/wyjÅ›cia

```
TRENING:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Iâ‚€   Iâ‚   Iâ‚‚   Iâ‚ƒ   Iâ‚„   Iâ‚…   Iâ‚†   Iâ‚‡   [Iâ‚ˆ]   Iâ‚‰   ...  Iâ‚â‚„  â”‚
â”‚  â†“    â†“    â†“    â†“    â†“    â†“    â†“    â†“     GT    â†“         â†“   â”‚
â”‚  Kontekst lewy (7 klatek)      â”‚      Kontekst prawy (6 klatek)â”‚
â”‚                                â–¼                               â”‚
â”‚                         ÃŽâ‚ˆ (predykcja)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INFERENCE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Iâ‚€   Iâ‚   Iâ‚‚   Iâ‚ƒ   Iâ‚„   Iâ‚…   Iâ‚†   Iâ‚‡    ?    Iâ‚‰   ...  Iâ‚â‚„  â”‚
â”‚                                â–¼                               â”‚
â”‚                         ÃŽâ‚ˆ (generowana)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Kluczowe parametry

| Parametr | WartoÅ›Ä‡ | Uwagi |
|----------|---------|-------|
| Liczba klatek wejÅ›ciowych | 15 | Iâ‚€ do Iâ‚â‚„ |
| Klatki przetwarzane (trening) | 14 | Bez Iâ‚ˆ (GT) |
| Klatki referencyjne | Iâ‚‡, Iâ‚‰ | NajbliÅ¼sze sÄ…siedztwo |
| Generowana klatka | Iâ‚ˆ | t = 0.5 miÄ™dzy Iâ‚‡ a Iâ‚‰ |
| RozdzielczoÅ›Ä‡ bazowa | 256Ã—256 | Trening |
| RozdzielczoÅ›Ä‡ docelowa | 256Ã—448 | Vimeo90K |

---

## 2. STAGE 1: Ekstrakcja cech (Feature Extraction)

### 2.1 Cel
Wydobycie wieloskalowych map cech z kaÅ¼dej z 14 klatek wejÅ›ciowych przy uÅ¼yciu wspÃ³Å‚dzielonego encodera konwolucyjnego. Dla klatek referencyjnych Iâ‚‡ i Iâ‚‰ dodatkowo ekstrahujemy cechy w peÅ‚nej rozdzielczoÅ›ci (s1) dla zachowania maksymalnej iloÅ›ci detali.

### 2.2 Architektura encodera

```
Input: Iâ‚– âˆˆ â„^(BÃ—3Ã—HÃ—W)

Encoder (wspÃ³Å‚dzielony, bazowany na RIFE):
â”œâ”€â”€ Conv2d(3 â†’ 32, k=3, s=1, p=1) + LeakyReLU
â”œâ”€â”€ Conv2d(32 â†’ 32, k=3, s=1, p=1) + LeakyReLU
â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â†’ Fâ‚–Ë¢Â¹ âˆˆ â„^(BÃ—32Ã—HÃ—W)        [TYLKO Iâ‚‡, Iâ‚‰]
â”œâ”€â”€ Conv2d(32 â†’ 64, k=3, s=2, p=1) + LeakyReLU
â”œâ”€â”€ Conv2d(64 â†’ 64, k=3, s=1, p=1) + LeakyReLU
â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â†’ Fâ‚–Ë¢â´ âˆˆ â„^(BÃ—128Ã—H/4Ã—W/4)   [TYLKO Iâ‚‡, Iâ‚‰]
â”œâ”€â”€ Conv2d(64 â†’ 128, k=3, s=2, p=1) + LeakyReLU
â”œâ”€â”€ Conv2d(128 â†’ 128, k=3, s=1, p=1) + LeakyReLU
â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â†’ Fâ‚–Ë¢â¸ âˆˆ â„^(BÃ—192Ã—H/8Ã—W/8)   [TYLKO Iâ‚‡, Iâ‚‰]
â”œâ”€â”€ Conv2d(128 â†’ 192, k=3, s=2, p=1) + LeakyReLU
â”œâ”€â”€ Conv2d(192 â†’ 192, k=3, s=1, p=1) + LeakyReLU
â””â”€â”€ Conv2d(192 â†’ 256, k=3, s=2, p=1) + LeakyReLU â†’ Fâ‚–Ë¢Â¹â¶ âˆˆ â„^(BÃ—256Ã—H/16Ã—W/16) [WSZYSTKIE]
```

**Uwaga:** Wczesne warstwy (przed s4) majÄ… mniej kanaÅ‚Ã³w (32â†’64) niÅ¼ w oryginalnej wersji, aby zoptymalizowaÄ‡ pamiÄ™Ä‡ przy zachowaniu cech s1.

### 2.3 Wymiary tensorÃ³w

| Skala | Wymiary | KanaÅ‚y | UÅ¼ycie | Klatki |
|-------|---------|--------|--------|--------|
| **s1 (1/1)** | **H Ã— W** | **32** | **STAGE 5 - full-res refinement** | **tylko Iâ‚‡, Iâ‚‰** |
| s4 (1/4) | H/4 Ã— W/4 | 128 | STAGE 3, STAGE 5 | tylko Iâ‚‡, Iâ‚‰ |
| s8 (1/8) | H/8 Ã— W/8 | 192 | STAGE 3 | tylko Iâ‚‡, Iâ‚‰ |
| s16 (1/16) | H/16 Ã— W/16 | 256 | STAGE 2 (transformer) | wszystkie 14 klatek |

### 2.4 Kodowanie pozycyjne (Positional Encoding)

```python
def sinusoidal_pe(k, C, max_len=15):
    """
    k: indeks klatki (0-14, z pominiÄ™ciem 8 podczas treningu)
    C: liczba kanaÅ‚Ã³w (32/128/192/256)
    
    Zachowujemy ORYGINALNE indeksy - model "wie" o brakujÄ…cej klatce 8
    """
    pe = zeros(C)
    for i in range(0, C, 2):
        pe[i] = sin(k / (10000 ** (i / C)))
        pe[i+1] = cos(k / (10000 ** (i / C)))
    return pe  # Broadcastowane do wymiarÃ³w przestrzennych
```

**Decyzja projektowa:** UÅ¼ywamy oryginalnych indeksÃ³w (0,1,...,7,9,...,14) zamiast renumeracji, aby model miaÅ‚ jawnÄ… informacjÄ™ o pozycji interpolowanego momentu.

### 2.5 WyjÅ›cia STAGE 1

```
Dla transformera (STAGE 2):
    F_temporal = [Fâ‚€Ë¢Â¹â¶, Fâ‚Ë¢Â¹â¶, ..., Fâ‚‡Ë¢Â¹â¶, Fâ‚‰Ë¢Â¹â¶, ..., Fâ‚â‚„Ë¢Â¹â¶]
    Wymiar: â„^(BÃ—14Ã—256Ã—H/16Ã—W/16)

Dla przepÅ‚ywu (STAGE 3):
    Fâ‚‡Ë¢â´, Fâ‚‰Ë¢â´ âˆˆ â„^(BÃ—128Ã—H/4Ã—W/4)
    Fâ‚‡Ë¢â¸, Fâ‚‰Ë¢â¸ âˆˆ â„^(BÃ—192Ã—H/8Ã—W/8)

Dla refinera peÅ‚nej rozdzielczoÅ›ci (STAGE 5):
    Fâ‚‡Ë¢Â¹, Fâ‚‰Ë¢Â¹ âˆˆ â„^(BÃ—32Ã—HÃ—W)           â† NOWE! PeÅ‚na rozdzielczoÅ›Ä‡
    Fâ‚‡Ë¢â´, Fâ‚‰Ë¢â´ âˆˆ â„^(BÃ—128Ã—H/4Ã—W/4)
```

### 2.6 Strategia zamraÅ¼ania wag

```
Epoki 1-10:  Encoder ZAMROÅ»ONY (wykorzystanie pretrenowanych wag RIFE)
Epoki 11+:  Stopniowe odmraÅ¼anie z niskim LR (lr_encoder = 0.1 Ã— lr_base)
```

**Uwaga:** Warstwy s1 (3â†’32â†’32) sÄ… NOWE i nie majÄ… pretrenowanych wag - naleÅ¼y je trenowaÄ‡ od poczÄ…tku lub zainicjalizowaÄ‡ z wag RIFE po dostosowaniu wymiarÃ³w.

### 2.7 Optymalizacja pamiÄ™ci

Cechy s1 sÄ… przechowywane TYLKO dla Iâ‚‡ i Iâ‚‰:
```python
# Pseudokod forward pass
for k, frame in enumerate(input_frames):
    f_s16 = encoder_full(frame)  # Zawsze do s16
    features_s16.append(f_s16)
    
    if k in [7, 9]:  # Tylko klatki referencyjne
        f_s1 = encoder.get_s1_features(frame)
        f_s4 = encoder.get_s4_features(frame)
        f_s8 = encoder.get_s8_features(frame)
        ref_features[k] = {'s1': f_s1, 's4': f_s4, 's8': f_s8}
```

### 2.8 TODO implementacyjne

- [ ] ZaimplementowaÄ‡ `FeatureEncoder` z czterema wyjÅ›ciami skalowymi (s1, s4, s8, s16)
- [ ] DodaÄ‡ `SinusoidalPositionalEncoding` z obsÅ‚ugÄ… nieciÄ…gÅ‚ych indeksÃ³w
- [ ] ZaÅ‚adowaÄ‡ pretrenowane wagi z RIFE (IFNet encoder) - dostosowaÄ‡ do nowych warstw s1
- [ ] Implementacja mechanizmu zamraÅ¼ania/odmraÅ¼ania
- [ ] Selektywne przechowywanie cech (s1 tylko dla Iâ‚‡, Iâ‚‰)
- [ ] Test pamiÄ™ci: 14Ã—256Ã—H/16Ã—W/16 + 2Ã—32Ã—HÃ—W

---

## 3. STAGE 2: Agregacja czasowa (Temporal Aggregation Transformer)

### 3.1 Cel
Modelowanie zaleÅ¼noÅ›ci czasowych miÄ™dzy 14 klatkami i agregacja do pojedynczej mapy kontekstowej F_ctx.

### 3.2 Kluczowa zmiana vs 64-klatkowa wersja

```
64 klatki: Okienkowa uwaga (W=8) â†’ O(TÂ·WÂ²) = O(64Â·64) = 4096 operacji
15 klatek: PEÅNA UWAGA moÅ¼liwa â†’ O(TÂ²) = O(14Â²) = 196 operacji

Redukcja: ~20Ã— mniej operacji! MoÅ¼na uÅ¼yÄ‡ peÅ‚nej uwagi bez okien.
```

### 3.3 Parametry transformera

| Parametr | WartoÅ›Ä‡ | Uzasadnienie |
|----------|---------|--------------|
| Liczba warstw L | 3 | KrÃ³tsza sekwencja = mniej warstw |
| Wymiar modelu D | 256 | Zgodny z kanaÅ‚ami s16 |
| Liczba gÅ‚Ã³w h | 8 | Standard dla D=256 |
| Uwaga czasowa | **PEÅNA** | T=14 pozwala na peÅ‚nÄ… uwagÄ™ |
| Rozmiar patcha P | 2Ã—2 | Tokenizacja przestrzenna |
| FFN expansion | 4Ã— | D â†’ 4D â†’ D |
| Dropout | 0.1 | Regularyzacja |

### 3.4 Tokenizacja przestrzenna

```
Input:  F_temporal âˆˆ â„^(BÃ—14Ã—256Ã—H/16Ã—W/16)

Dla H=W=256:
    Spatial size @ s16: 16Ã—16
    Patch size: 2Ã—2
    Tokens per frame: (16/2)Ã—(16/2) = 64
    
Output po patchify:
    tokens âˆˆ â„^(BÃ—14Ã—64Ã—256)  czyli (B, T, L, D)
    gdzie T=14 (klatki), L=64 (patche przestrzenne), D=256 (wymiar)
```

### 3.5 Architektura warstwy transformera

```
Dla kaÅ¼dej z L=3 warstw:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. TEMPORAL SELF-ATTENTION (peÅ‚na, nie okienkowa!)             â”‚
â”‚    â”œâ”€â”€ Input: (B, T, L, D) â†’ reshape â†’ (BÃ—L, T, D)             â”‚
â”‚    â”œâ”€â”€ MultiHeadAttention(D, heads=8)                          â”‚
â”‚    â”œâ”€â”€ KaÅ¼dy patch "widzi" wszystkie 14 klatek                 â”‚
â”‚    â””â”€â”€ Output: (BÃ—L, T, D) â†’ reshape â†’ (B, T, L, D)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. SPATIAL PROCESSING (DepthwiseSeparable Conv)                â”‚
â”‚    â”œâ”€â”€ Reshape: (B, T, L, D) â†’ (BÃ—T, D, 8, 8)                  â”‚
â”‚    â”œâ”€â”€ DepthwiseConv2d(D, D, k=3, groups=D)                    â”‚
â”‚    â”œâ”€â”€ PointwiseConv2d(D, D, k=1)                              â”‚
â”‚    â”œâ”€â”€ GroupNorm(8 groups) + residual                          â”‚
â”‚    â””â”€â”€ Reshape back: (BÃ—T, D, 8, 8) â†’ (B, T, L, D)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. FEED-FORWARD NETWORK                                        â”‚
â”‚    â”œâ”€â”€ LayerNorm                                               â”‚
â”‚    â”œâ”€â”€ Linear(D â†’ 4D) + GELU                                   â”‚
â”‚    â”œâ”€â”€ Linear(4D â†’ D)                                          â”‚
â”‚    â”œâ”€â”€ Dropout(0.1)                                            â”‚
â”‚    â””â”€â”€ Residual connection                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.6 Adaptacyjna agregacja czasowa

```
Po 3 warstwach transformera:
    F_agg âˆˆ â„^(BÃ—14Ã—64Ã—256)

Agregacja do pojedynczego kontekstu:

1. Global Average Pooling per klatka:
   gâ‚– = mean(F_agg[k], dim=spatial) âˆˆ â„^D    dla k âˆˆ {0..7, 9..14}

2. Importance scoring (MLP):
   Î±â‚–_raw = MLP([gâ‚€, gâ‚, ..., gâ‚‡, gâ‚‰, ..., gâ‚â‚„])
   MLP: D â†’ D/4 â†’ 1 (per klatka)

3. Softmax normalization:
   Î±â‚– = softmax(Î±_raw)    gdzie Î£Î±â‚– = 1

4. Weighted aggregation:
   F_ctx = Î£â‚– Î±â‚– Â· F_agg[k] âˆˆ â„^(BÃ—256Ã—H/16Ã—W/16)
```

**Output pomocniczy:** Wektor wag Î±â‚– do wizualizacji (ktÃ³re klatki model uznaÅ‚ za najwaÅ¼niejsze).

### 3.7 WyjÅ›cie STAGE 2

```
F_ctx âˆˆ â„^(BÃ—256Ã—H/16Ã—W/16)

Oczekiwane zachowanie wag Î±:
- WyÅ¼sze dla klatek bliskich t=8 (czyli Iâ‚‡, Iâ‚‰)
- WyÅ¼sze dla klatek z istotnymi zdarzeniami ruchu
- NiÅ¼sze dla klatek statycznych/redundantnych
```

### 3.8 TODO implementacyjne

- [ ] ZaimplementowaÄ‡ `TemporalTransformer` z peÅ‚nÄ… uwagÄ… (nie okienkowÄ…!)
- [ ] `PatchEmbedding` - konwersja map cech na tokeny
- [ ] `DepthwiseSeparableConv2d` dla przetwarzania przestrzennego
- [ ] `AdaptiveTemporalAggregation` z MLP do waÅ¼enia
- [ ] ZachowaÄ‡ Î±â‚– jako output do TensorBoard/wizualizacji
- [ ] Test: porÃ³wnanie pamiÄ™ci peÅ‚na vs okienkowa uwaga

---

## 4. STAGE 3: Wieloskalowe szacowanie przepÅ‚ywu (Flow Estimation)

### 4.1 Cel
Oszacowanie przepÅ‚ywÃ³w optycznych i map okluzji dla klatki Iâ‚ˆ wzglÄ™dem Iâ‚‡ i Iâ‚‰.

### 4.2 Architektura kaskady 2-skalowej

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   SKALA s8  â”‚
                    â”‚  (gruba)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                  â–¼                  â–¼
   flowâ‚‡Ë¢â¸           flowâ‚‰Ë¢â¸          logit_Oâ‚‡Ë¢â¸, logit_Oâ‚‰Ë¢â¸
   âˆˆ â„^(BÃ—2Ã—H/8Ã—W/8)                  âˆˆ â„^(BÃ—1Ã—H/8Ã—W/8)
        â”‚                  â”‚                  â”‚
        â”‚    Ã—2 upsample + scale              â”‚    bilinear upsample
        â–¼                  â–¼                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   SKALA s4  â”‚
                    â”‚ (refinement)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                  â–¼                  â–¼
   flowâ‚‡Ë¢â´           flowâ‚‰Ë¢â´           Oâ‚‡Ë¢â´, Oâ‚‰Ë¢â´
   âˆˆ â„^(BÃ—2Ã—H/4Ã—W/4)                  âˆˆ [0,1]^(BÃ—1Ã—H/4Ã—W/4)
```

### 4.3 Skala s8 - szacowanie grube

**WejÅ›cie:**
```
Fâ‚‡Ë¢â¸ âˆˆ â„^(BÃ—192Ã—H/8Ã—W/8)      - cechy klatki Iâ‚‡
Fâ‚‰Ë¢â¸ âˆˆ â„^(BÃ—192Ã—H/8Ã—W/8)      - cechy klatki Iâ‚‰
F_ctxË¢â¸ = AvgPool2d(F_ctx)     - kontekst âˆˆ â„^(BÃ—256Ã—H/8Ã—W/8)
t_chan = 0.5 Â· ðŸ™              - czas âˆˆ â„^(BÃ—1Ã—H/8Ã—W/8)

input_s8 = concat([Fâ‚‡Ë¢â¸, Fâ‚‰Ë¢â¸, F_ctxË¢â¸, t_chan])
         âˆˆ â„^(BÃ—641Ã—H/8Ã—W/8)
```

**SieÄ‡ IFNet-like:**
```
Conv(641 â†’ 256, k=3) + LeakyReLU
ResBlock(256) Ã— 3
Conv(256 â†’ 6, k=3)  # 2+2+1+1 = 6 kanaÅ‚Ã³w wyjÅ›ciowych

Output:
â”œâ”€â”€ flowâ‚‡Ë¢â¸ âˆˆ â„^(BÃ—2Ã—H/8Ã—W/8)      - przepÅ‚yw do Iâ‚‡
â”œâ”€â”€ flowâ‚‰Ë¢â¸ âˆˆ â„^(BÃ—2Ã—H/8Ã—W/8)      - przepÅ‚yw do Iâ‚‰
â”œâ”€â”€ logit_Oâ‚‡Ë¢â¸ âˆˆ â„^(BÃ—1Ã—H/8Ã—W/8)   - logit okluzji Iâ‚‡
â””â”€â”€ logit_Oâ‚‰Ë¢â¸ âˆˆ â„^(BÃ—1Ã—H/8Ã—W/8)   - logit okluzji Iâ‚‰
```

### 4.4 Skala s4 - refinement

**Upsampling z s8:**
```python
flowâ‚‡_up = 2 Ã— bilinear_upsample(flowâ‚‡Ë¢â¸, scale=2)  # Ã—2 bo wiÄ™ksza rozdzielczoÅ›Ä‡
flowâ‚‰_up = 2 Ã— bilinear_upsample(flowâ‚‰Ë¢â¸, scale=2)
logit_Oâ‚‡_up = bilinear_upsample(logit_Oâ‚‡Ë¢â¸, scale=2)  # bez skalowania wartoÅ›ci
logit_Oâ‚‰_up = bilinear_upsample(logit_Oâ‚‰Ë¢â¸, scale=2)
```

**WejÅ›cie do refinera:**
```
Fâ‚‡Ë¢â´ âˆˆ â„^(BÃ—128Ã—H/4Ã—W/4)
Fâ‚‰Ë¢â´ âˆˆ â„^(BÃ—128Ã—H/4Ã—W/4)
F_ctxË¢â´ = bilinear_upsample(F_ctx, scale=4) âˆˆ â„^(BÃ—256Ã—H/4Ã—W/4)
t_chan_s4 = 0.5 Â· ðŸ™ âˆˆ â„^(BÃ—1Ã—H/4Ã—W/4)

refine_input = concat([Fâ‚‡Ë¢â´, Fâ‚‰Ë¢â´, F_ctxË¢â´, 
                       flowâ‚‡_up, flowâ‚‰_up,
                       logit_Oâ‚‡_up, logit_Oâ‚‰_up,
                       t_chan_s4])
             âˆˆ â„^(BÃ—519Ã—H/4Ã—W/4)
```

**SieÄ‡ refinujÄ…ca:**
```
Conv(519 â†’ 128, k=3) + LeakyReLU
ResBlock(128) Ã— 2
Conv(128 â†’ 6, k=3)  # delta dla flow i logit

Output (residualne!):
â”œâ”€â”€ Î”flowâ‚‡ âˆˆ â„^(BÃ—2Ã—H/4Ã—W/4)
â”œâ”€â”€ Î”flowâ‚‰ âˆˆ â„^(BÃ—2Ã—H/4Ã—W/4)
â”œâ”€â”€ Î”logit_Oâ‚‡ âˆˆ â„^(BÃ—1Ã—H/4Ã—W/4)
â””â”€â”€ Î”logit_Oâ‚‰ âˆˆ â„^(BÃ—1Ã—H/4Ã—W/4)

Finalne wartoÅ›ci:
flowâ‚‡Ë¢â´ = flowâ‚‡_up + Î”flowâ‚‡
flowâ‚‰Ë¢â´ = flowâ‚‰_up + Î”flowâ‚‰
logit_Oâ‚‡Ë¢â´ = logit_Oâ‚‡_up + Î”logit_Oâ‚‡
logit_Oâ‚‰Ë¢â´ = logit_Oâ‚‰_up + Î”logit_Oâ‚‰

Sigmoid NA KOÅƒCU:
Oâ‚‡Ë¢â´ = Ïƒ(logit_Oâ‚‡Ë¢â´) âˆˆ [0,1]^(BÃ—1Ã—H/4Ã—W/4)
Oâ‚‰Ë¢â´ = Ïƒ(logit_Oâ‚‰Ë¢â´) âˆˆ [0,1]^(BÃ—1Ã—H/4Ã—W/4)
```

### 4.5 WyjÅ›cie STAGE 3

```
flowâ‚‡Ë¢â´ âˆˆ â„^(BÃ—2Ã—H/4Ã—W/4)      - przepÅ‚yw optyczny Iâ‚ˆâ†’Iâ‚‡
flowâ‚‰Ë¢â´ âˆˆ â„^(BÃ—2Ã—H/4Ã—W/4)      - przepÅ‚yw optyczny Iâ‚ˆâ†’Iâ‚‰
Oâ‚‡Ë¢â´ âˆˆ [0,1]^(BÃ—1Ã—H/4Ã—W/4)     - mapa okluzji dla Iâ‚‡
Oâ‚‰Ë¢â´ âˆˆ [0,1]^(BÃ—1Ã—H/4Ã—W/4)     - mapa okluzji dla Iâ‚‰
```

### 4.6 TODO implementacyjne

- [ ] ZaimplementowaÄ‡ `FlowEstimatorS8` (IFNet-like)
- [ ] ZaimplementowaÄ‡ `FlowRefinerS4` (ResBlocks)
- [ ] `ResBlock` z GroupNorm
- [ ] Upsampling z prawidÅ‚owym skalowaniem flow (Ã—2)
- [ ] Sigmoid TYLKO na koÅ„cu (logity w Å›rodku!)
- [ ] Wizualizacja flow jako color wheel

---

## 5. STAGE 4: Synteza klatki zgrubnej (Coarse Synthesis)

### 5.1 Cel
Wygenerowanie zgrubnej klatki poÅ›redniej Iâ‚ˆ^coarse w rozdzielczoÅ›ci s4.

### 5.2 Backward Warping

```python
# Downscale klatek referencyjnych
Iâ‚‡Ë¢â´ = bilinear_downsample(Iâ‚‡, scale=0.25)  # âˆˆ â„^(BÃ—3Ã—H/4Ã—W/4)
Iâ‚‰Ë¢â´ = bilinear_downsample(Iâ‚‰, scale=0.25)

# Backward warp - przenosimy piksele z Iâ‚‡/Iâ‚‰ do pozycji Iâ‚ˆ
Iâ‚ˆ_from_7 = backward_warp(Iâ‚‡Ë¢â´, flowâ‚‡Ë¢â´)  # grid_sample z flow
Iâ‚ˆ_from_9 = backward_warp(Iâ‚‰Ë¢â´, flowâ‚‰Ë¢â´)
```

### 5.3 Occlusion-aware Blending

```python
# WaÅ¼one Å‚Ä…czenie z uÅ¼yciem map okluzji
Îµ = 1e-8
Iâ‚ˆ_blend = (Oâ‚‡Ë¢â´ * Iâ‚ˆ_from_7 + Oâ‚‰Ë¢â´ * Iâ‚ˆ_from_9) / (Oâ‚‡Ë¢â´ + Oâ‚‰Ë¢â´ + Îµ)

# Iâ‚ˆ_blend âˆˆ â„^(BÃ—3Ã—H/4Ã—W/4)
```

**Interpretacja:**
- Wysoka Oâ‚‡ â†’ region dobrze widoczny w Iâ‚‡ â†’ wiÄ™cej wagi z Iâ‚‡
- Wysoka Oâ‚‰ â†’ region dobrze widoczny w Iâ‚‰ â†’ wiÄ™cej wagi z Iâ‚‰
- Obie niskie â†’ okluzja w obu â†’ Å›rednia (lub potrzebny inpainting)

### 5.4 Context Injection (ContextNet)

```
Input:
â”œâ”€â”€ Iâ‚ˆ_blend âˆˆ â„^(BÃ—3Ã—H/4Ã—W/4)
â””â”€â”€ F_ctxË¢â´ âˆˆ â„^(BÃ—256Ã—H/4Ã—W/4)

ctx_input = concat([Iâ‚ˆ_blend, F_ctxË¢â´]) âˆˆ â„^(BÃ—259Ã—H/4Ã—W/4)

ContextNet (bardzo lekka!):
â”œâ”€â”€ Conv(259 â†’ 64, k=3, p=1) + ReLU
â””â”€â”€ Conv(64 â†’ 3, k=3, p=1)           # residual w przestrzeni obrazu

Output:
residual âˆˆ â„^(BÃ—3Ã—H/4Ã—W/4)
Iâ‚ˆ_coarse = Iâ‚ˆ_blend + residual
```

### 5.5 WyjÅ›cie STAGE 4

```
Iâ‚ˆ_coarse âˆˆ â„^(BÃ—3Ã—H/4Ã—W/4)

Klatka zgrubna ale czasowo spÃ³jna - gÅ‚Ã³wna praca semantyczna
wykonana przez transformer, tu tylko dopasowanie szczegÃ³Å‚Ã³w.
```

### 5.6 TODO implementacyjne

- [ ] ZaimplementowaÄ‡ `backward_warp` uÅ¼ywajÄ…c `F.grid_sample`
- [ ] `OcclusionBlender` z obsÅ‚ugÄ… epsilon
- [ ] `ContextNet` (2 warstwy conv)
- [ ] Test: czy blend wyglÄ…da sensownie przed ContextNet?

---

## 6. STAGE 5: Refinement peÅ‚nej rozdzielczoÅ›ci (Full-res Refinement)

### 6.1 Cel
Dopracowanie detali w peÅ‚nej rozdzielczoÅ›ci HÃ—W z wykorzystaniem cech s1 z klatek referencyjnych.

### 6.2 Upsample klatki zgrubnej

```python
Iâ‚ˆ_up = bilinear_upsample(Iâ‚ˆ_coarse, scale=4)  # âˆˆ â„^(BÃ—3Ã—HÃ—W)
```

### 6.3 Wykorzystanie cech peÅ‚nej rozdzielczoÅ›ci (s1)

**KLUCZOWA ZMIANA:** Zamiast upsamplowaÄ‡ cechy s4, uÅ¼ywamy bezpoÅ›rednio cech s1 - zero utraty informacji!

```python
# Cechy s1 juÅ¼ sÄ… w peÅ‚nej rozdzielczoÅ›ci - nie trzeba upsamplowaÄ‡!
Fâ‚‡Ë¢Â¹ âˆˆ â„^(BÃ—32Ã—HÃ—W)  # bezpoÅ›rednio z STAGE 1
Fâ‚‰Ë¢Â¹ âˆˆ â„^(BÃ—32Ã—HÃ—W)  # bezpoÅ›rednio z STAGE 1

# Nie ma redukcji kanaÅ‚Ã³w ani upsamplingu - peÅ‚ne detale zachowane!
```

**PorÃ³wnanie z poprzedniÄ… wersjÄ…:**
```
STARA WERSJA (bez s1):
Fâ‚‡Ë¢â´ âˆˆ â„^(BÃ—128Ã—H/4Ã—W/4) â†’ Conv1x1 â†’ â„^(BÃ—32Ã—H/4Ã—W/4) â†’ upsample Ã—4 â†’ â„^(BÃ—32Ã—HÃ—W)
                                      â†‘ utrata informacji przez upsample!

NOWA WERSJA (z s1):
Fâ‚‡Ë¢Â¹ âˆˆ â„^(BÃ—32Ã—HÃ—W) â†’ bezpoÅ›rednio do refinera
                      â†‘ peÅ‚ne detale, zero utraty!
```

### 6.4 Lightweight Refinement Network

```
Input:
refine_input = concat([Iâ‚ˆ_up, Fâ‚‡Ë¢Â¹, Fâ‚‰Ë¢Â¹]) âˆˆ â„^(BÃ—67Ã—HÃ—W)
                       â†‘      â†‘     â†‘
                       3     32    32  = 67 kanaÅ‚Ã³w

RefineNet:
â”œâ”€â”€ Conv(67 â†’ 64, k=3, p=1) + GroupNorm(8) + ReLU
â”œâ”€â”€ ResBlock(64 â†’ 64) Ã— 2
â”‚   â””â”€â”€ Conv(64â†’64, k=3) + GN + ReLU + Conv(64â†’64, k=3) + GN + residual
â”œâ”€â”€ Conv(64 â†’ 32, k=3, p=1) + GroupNorm(4) + ReLU
â””â”€â”€ Conv(32 â†’ 3, k=3, p=1)  # bez aktywacji - residual

Output:
residual âˆˆ â„^(BÃ—3Ã—HÃ—W)
Iâ‚ˆ_final = Iâ‚ˆ_up + residual
```

### 6.5 WyjÅ›cie koÅ„cowe

```
Iâ‚ˆ_final âˆˆ â„^(BÃ—3Ã—HÃ—W)  - finalna interpolowana klatka

Clamp do [0, 1] przed zapisem/wizualizacjÄ…!
```

### 6.6 Zalety uÅ¼ycia s1

| Aspekt | Bez s1 (stara wersja) | Z s1 (nowa wersja) |
|--------|----------------------|-------------------|
| RozdzielczoÅ›Ä‡ cech | H/4 Ã— W/4 â†’ upsample | H Ã— W (natywna) |
| Utrata detali | Tak (przez upsample) | Nie |
| KanaÅ‚y | 128 â†’ 32 (redukcja) | 32 (od razu) |
| PamiÄ™Ä‡ | Mniej | +~16MB (akceptowalne) |
| JakoÅ›Ä‡ krawÄ™dzi | Rozmyte | **Ostre** |

### 6.7 TODO implementacyjne

- [ ] ZmodyfikowaÄ‡ `FullResRefiner` na przyjmowanie Fâ‚‡Ë¢Â¹, Fâ‚‰Ë¢Â¹ zamiast upsamplowanych s4
- [ ] UsunÄ…Ä‡ `ChannelReducer` i upsampling dla s4 (niepotrzebne!)
- [ ] ResBlocks z GroupNorm
- [ ] SprawdziÄ‡ czy final output jest w [0,1]
- [ ] PorÃ³wnanie wizualne: Iâ‚ˆ_coarse vs Iâ‚ˆ_final vs GT
- [ ] **Test A/B:** wersja z s1 vs bez s1 - spodziewana poprawa na krawÄ™dziach

---

## 7. Funkcje strat (Loss Functions)

### 7.1 GÅ‚Ã³wne straty

```python
# L1 Reconstruction Loss
L_rec = L1(Iâ‚ˆ_final, Iâ‚ˆ_GT)

# Perceptual Loss (LPIPS lub VGG)
L_perc = LPIPS(Iâ‚ˆ_final, Iâ‚ˆ_GT)  # lub VGG feature matching

# Total Loss
L_total = L_rec + Î»_perc Ã— L_perc

# Sugerowane wagi:
Î»_perc = 0.1
```

### 7.2 Opcjonalne straty dodatkowe

```python
# Census Loss (dla robustnoÅ›ci na zmiany oÅ›wietlenia)
L_census = census_transform_loss(Iâ‚ˆ_final, Iâ‚ˆ_GT)

# Flow Smoothness (regularyzacja przepÅ‚ywu)
L_smooth = smoothness_loss(flowâ‚‡Ë¢â´) + smoothness_loss(flowâ‚‰Ë¢â´)

# Warping Loss (czy warped frames sÄ… sensowne)
L_warp = L1(Iâ‚ˆ_from_7, Iâ‚ˆ_GT) + L1(Iâ‚ˆ_from_9, Iâ‚ˆ_GT)
```

### 7.3 TODO implementacyjne

- [ ] Implementacja L1 + LPIPS
- [ ] RozwaÅ¼yÄ‡ Census Loss dla trudnych scen
- [ ] Logowanie poszczegÃ³lnych skÅ‚adnikÃ³w loss do TensorBoard

---

## 8. Pipeline treningu

### 8.1 Przygotowanie danych

```python
# Wczytanie 15-klatkowego klipu
frames = load_clip(video, start_idx, length=15)  # [Iâ‚€, ..., Iâ‚â‚„]

# Separacja
input_frames = frames[[0,1,2,3,4,5,6,7,9,10,11,12,13,14]]  # 14 klatek
gt_frame = frames[8]  # Ground Truth

# Augmentacje
# - Random crop do 256Ã—256
# - Random horizontal flip
# - Random temporal flip (odwrÃ³cenie kolejnoÅ›ci)
# - Color jitter (ostroÅ¼nie!)
```

### 8.2 Forward pass

```python
def forward(input_frames, t=0.5):
    # STAGE 1 - ekstrakcja cech
    features_s16 = []
    ref_features = {}
    
    for k, frame in enumerate(input_frames):
        f_s16 = encoder.forward_s16(frame)
        features_s16.append(f_s16)
        
        if k in [7, 9]:  # Klatki referencyjne
            ref_features[k] = {
                's1': encoder.get_s1(frame),   # â† NOWE! PeÅ‚na rozdzielczoÅ›Ä‡
                's4': encoder.get_s4(frame),
                's8': encoder.get_s8(frame),
            }
    
    # STAGE 2 - agregacja czasowa
    F_ctx, alphas = temporal_transformer(torch.stack(features_s16, dim=1))
    
    # STAGE 3 - szacowanie przepÅ‚ywu
    flows, occlusions = flow_estimator(
        ref_features[7]['s8'], ref_features[9]['s8'],
        ref_features[7]['s4'], ref_features[9]['s4'],
        F_ctx, t
    )
    
    # STAGE 4 - synteza zgrubna
    I_coarse = coarse_synthesis(
        input_frames[7], input_frames[9],  # Iâ‚‡, Iâ‚‰
        flows, occlusions, F_ctx
    )
    
    # STAGE 5 - refinement z cechami s1
    I_final = full_res_refiner(
        I_coarse,
        ref_features[7]['s1'],  # â† NOWE! Cechy peÅ‚nej rozdzielczoÅ›ci
        ref_features[9]['s1']   # â† NOWE!
    )
    
    return I_final, alphas, flows, occlusions
```

### 8.3 Harmonogram treningu

| Etap | Epoki | Co trenujemy | LR | Uwagi |
|------|-------|--------------|-----|-------|
| 1 | 1-10 | Wszystko POZA encoderem | 1e-4 | Encoder zamroÅ¼ony |
| 2 | 11-30 | Wszystko | 1e-4 (encoder: 1e-5) | Stopniowe odmraÅ¼anie |
| 3 | 31-50 | Wszystko | 1e-5 | Fine-tuning |

**Uwaga:** Warstwy s1 (3â†’32â†’32) sÄ… nowe i mogÄ… wymagaÄ‡ wyÅ¼szego LR na poczÄ…tku.

### 8.4 TODO implementacyjne

- [ ] Dataloader dla 15-klatkowych klipÃ³w (Vimeo90K septuplet â†’ rozszerzyÄ‡?)
- [ ] Augmentacje z temporal awareness
- [ ] Training loop z gradient accumulation (jeÅ›li batch nie mieÅ›ci siÄ™)
- [ ] Checkpointing co N epok
- [ ] TensorBoard: loss, Î±â‚–, przykÅ‚adowe interpolacje

---

## 9. Ewaluacja

### 9.1 Metryki

| Metryka | Co mierzy | Target |
|---------|-----------|--------|
| PSNR | Pixel-level similarity | >30 dB |
| SSIM | Structural similarity | >0.9 |
| LPIPS | Perceptual similarity | <0.1 |
| IE (Interpolation Error) | Flow accuracy | - |

### 9.2 Benchmarki

- **Vimeo90K-septuplet** (podstawowy)
- **UCF101** (action recognition clips)
- **DAVIS** (z okluzjami)
- **SNU-FILM** (rÃ³Å¼ne poziomy trudnoÅ›ci: Easy/Medium/Hard/Extreme)

### 9.3 Ablacje do przeprowadzenia

1. **Liczba klatek kontekstu:** 3 vs 7 vs 15 klatek
2. **PeÅ‚na vs okienkowa uwaga:** czy peÅ‚na jest lepsza przy T=14?
3. **Z kontekstem vs bez:** czy F_ctx pomaga?
4. **Liczba warstw transformera:** 2 vs 3 vs 4
5. **Z s1 vs bez s1:** czy cechy peÅ‚nej rozdzielczoÅ›ci poprawiajÄ… detale? â† NOWE!

---

## 10. Szacowanie zasobÃ³w

### 10.1 PamiÄ™Ä‡ GPU (szacunki dla batch=4, 256Ã—256)

| Komponent | PamiÄ™Ä‡ | Uwagi |
|-----------|--------|-------|
| 14 klatek input | ~50 MB | |
| Features s16 (14Ã—256Ã—16Ã—16) | ~28 MB | |
| **Features s1 (2Ã—32Ã—256Ã—256)** | **~16 MB** | **NOWE** |
| Features s4/s8 (tylko Iâ‚‡, Iâ‚‰) | ~10 MB | |
| Transformer activations | ~200 MB | |
| Flow estimation | ~100 MB | |
| Refinement (full-res) | ~200 MB | WiÄ™ksze przez s1 |
| **ÅÄ…cznie (forward)** | **~650 MB** | |
| **Z gradientami (training)** | **~3-4 GB** | |

### 10.2 PorÃ³wnanie wersji

```
64 klatki (oryginaÅ‚):     ~8-12 GB VRAM
15 klatek (bez s1):       ~2-3 GB VRAM
15 klatek (z s1):         ~3-4 GB VRAM    â† AKTUALNA WERSJA

Wzrost przez s1: ~0.5-1 GB (akceptowalny trade-off za lepsze detale!)
```

### 10.3 Optymalizacje pamiÄ™ci (jeÅ›li potrzebne)

```python
# Gradient checkpointing dla transformera
# (przeliczyÄ‡ aktywacje podczas backward zamiast je trzymaÄ‡)
from torch.utils.checkpoint import checkpoint

# Mixed precision training
scaler = torch.cuda.amp.GradScaler()
with torch.cuda.amp.autocast():
    output = model(input)
```

---

## 11. Struktura kodu (proponowana)

```
lift/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ encoder.py          # STAGE 1: FeatureEncoder (z wyjÅ›ciem s1!)
â”‚   â”œâ”€â”€ transformer.py      # STAGE 2: TemporalTransformer
â”‚   â”œâ”€â”€ flow_estimator.py   # STAGE 3: FlowEstimator
â”‚   â”œâ”€â”€ synthesis.py        # STAGE 4: CoarseSynthesis
â”‚   â”œâ”€â”€ refiner.py          # STAGE 5: FullResRefiner (przyjmuje s1!)
â”‚   â””â”€â”€ lift.py             # Full LIFT model
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vimeo_dataset.py    # Vimeo90K loader (15 frames)
â”‚   â””â”€â”€ augmentations.py    # Temporal-aware augmentations
â”œâ”€â”€ losses/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ reconstruction.py   # L1, L2
â”‚   â”œâ”€â”€ perceptual.py       # LPIPS, VGG
â”‚   â””â”€â”€ flow_losses.py      # Smoothness, warping
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ warp.py             # backward_warp, grid_sample
â”‚   â”œâ”€â”€ positional.py       # Sinusoidal PE
â”‚   â””â”€â”€ visualization.py    # Flow vis, attention vis
â”œâ”€â”€ train.py
â”œâ”€â”€ evaluate.py
â””â”€â”€ config.yaml
```

---

## 12. Checklisty

### 12.1 Przed rozpoczÄ™ciem implementacji

- [ ] PotwierdziÄ‡ dostÄ™p do Vimeo90K dataset
- [ ] SprawdziÄ‡ dostÄ™pnoÅ›Ä‡ GPU (min. 8GB VRAM zalecane)
- [ ] ZainstalowaÄ‡ zaleÅ¼noÅ›ci: torch, torchvision, lpips, tensorboard
- [ ] PobraÄ‡ pretrenowane wagi RIFE

### 12.2 Milestone 1: Encoder + Transformer

- [ ] STAGE 1 dziaÅ‚a z wyjÅ›ciami s1, s4, s8, s16
- [ ] Cechy s1 tylko dla Iâ‚‡, Iâ‚‰ (optymalizacja pamiÄ™ci)
- [ ] STAGE 2 dziaÅ‚a, Î±â‚– sumujÄ… siÄ™ do 1
- [ ] Forward pass bez bÅ‚Ä™dÃ³w pamiÄ™ci

### 12.3 Milestone 2: Flow + Synthesis

- [ ] STAGE 3 produkuje sensowne flow (wizualizacja)
- [ ] STAGE 4 backward warp dziaÅ‚a poprawnie
- [ ] I_coarse wyglÄ…da jak rozmyta interpolacja

### 12.4 Milestone 3: Full Pipeline z s1

- [ ] STAGE 5 przyjmuje Fâ‚‡Ë¢Â¹, Fâ‚‰Ë¢Â¹ bezpoÅ›rednio
- [ ] Loss spada podczas treningu
- [ ] Wizualnie wyniki sÄ… sensowne
- [ ] KrawÄ™dzie sÄ… ostrzejsze niÅ¼ w wersji bez s1

### 12.5 Milestone 4: Ewaluacja

- [ ] PSNR/SSIM/LPIPS na validation set
- [ ] PorÃ³wnanie z RIFE baseline
- [ ] Ablacja: z s1 vs bez s1
- [ ] PozostaÅ‚e ablacje przeprowadzone

---

## 13. Diagram przepÅ‚ywu danych (podsumowanie)

```
INPUT: [Iâ‚€, Iâ‚, ..., Iâ‚‡, Iâ‚‰, ..., Iâ‚â‚„]  (14 klatek)
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: Feature Extraction                                        â”‚
â”‚                                                                     â”‚
â”‚   Wszystkie 14 klatek â†’ s16 features                               â”‚
â”‚   Tylko Iâ‚‡, Iâ‚‰ â†’ s1, s4, s8 features                               â”‚
â”‚                                                                     â”‚
â”‚   Output:                                                           â”‚
â”‚   â”œâ”€â”€ F_temporal: [B, 14, 256, H/16, W/16]                         â”‚
â”‚   â”œâ”€â”€ Fâ‚‡Ë¢Â¹, Fâ‚‰Ë¢Â¹: [B, 32, H, W]           â† FULL RESOLUTION       â”‚
â”‚   â”œâ”€â”€ Fâ‚‡Ë¢â´, Fâ‚‰Ë¢â´: [B, 128, H/4, W/4]                               â”‚
â”‚   â””â”€â”€ Fâ‚‡Ë¢â¸, Fâ‚‰Ë¢â¸: [B, 192, H/8, W/8]                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: Temporal Transformer                                       â”‚
â”‚                                                                     â”‚
â”‚   F_temporal â†’ Full Self-Attention (T=14) â†’ Adaptive Aggregation   â”‚
â”‚                                                                     â”‚
â”‚   Output:                                                           â”‚
â”‚   â”œâ”€â”€ F_ctx: [B, 256, H/16, W/16]                                  â”‚
â”‚   â””â”€â”€ Î±â‚–: [14] (attention weights)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: Flow Estimation (2-scale cascade)                         â”‚
â”‚                                                                     â”‚
â”‚   [Fâ‚‡Ë¢â¸, Fâ‚‰Ë¢â¸, F_ctx] â†’ s8 estimation â†’ s4 refinement             â”‚
â”‚                                                                     â”‚
â”‚   Output:                                                           â”‚
â”‚   â”œâ”€â”€ flowâ‚‡Ë¢â´, flowâ‚‰Ë¢â´: [B, 2, H/4, W/4]                          â”‚
â”‚   â””â”€â”€ Oâ‚‡Ë¢â´, Oâ‚‰Ë¢â´: [B, 1, H/4, W/4]                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 4: Coarse Synthesis                                           â”‚
â”‚                                                                     â”‚
â”‚   [Iâ‚‡, Iâ‚‰, flows, occlusions, F_ctx] â†’ warp + blend + context      â”‚
â”‚                                                                     â”‚
â”‚   Output:                                                           â”‚
â”‚   â””â”€â”€ Iâ‚ˆ_coarse: [B, 3, H/4, W/4]                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 5: Full-Resolution Refinement                                 â”‚
â”‚                                                                     â”‚
â”‚   [Iâ‚ˆ_coarseâ†‘, Fâ‚‡Ë¢Â¹, Fâ‚‰Ë¢Â¹] â†’ RefineNet â†’ residual                  â”‚
â”‚                    â†‘                                                â”‚
â”‚          FULL-RES FEATURES (no upsampling loss!)                   â”‚
â”‚                                                                     â”‚
â”‚   Output:                                                           â”‚
â”‚   â””â”€â”€ Iâ‚ˆ_final: [B, 3, H, W]                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
              OUTPUT: ÃŽâ‚ˆ
              
              Loss = L1(ÃŽâ‚ˆ, Iâ‚ˆ_GT) + Î»Â·LPIPS(ÃŽâ‚ˆ, Iâ‚ˆ_GT)
```

---
